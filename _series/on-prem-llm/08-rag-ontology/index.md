---
title: "RAG + Ontology 통합 - 지능적 검색의 완성"
permalink: /series/on-prem-llm/08-rag-ontology/
date: 2026-02-06
excerpt: "벡터 검색과 그래프 탐색을 결합한 하이브리드 RAG 아키텍처"
categories:
  - AI-ML
tags:
  - RAG
  - Ontology
  - Graph-RAG
  - Hybrid Search
series: "온프레미스 LLM 구축"
series_order: 8
---

## 소개

여기가 이 연재의 **클라이막스**입니다. RAG의 검색 능력 + Ontology의 관계 추론 능력을 합치면, 단순한 질의응답을 넘어 **"진짜 지능적인 검색"**이 가능해집니다.

다만, 모든 프로젝트에 필요한 건 아닙니다. "다단계 관계 추론이 필요한가?"에 대한 답이 "예"일 때만 도입하세요.

---

## 하이브리드 검색 아키텍처

```
사용자 질문
    ├─ 벡터 검색 (RAG) → 관련 문서 Top-K
    ├─ 그래프 탐색 (Ontology) → 관련 Entity + Relation
    └─ 결합 → 리랭킹 → LLM에 전달 → 답변
```

## Graph-RAG 패턴

Knowledge Graph를 RAG의 검색 소스로 활용하는 패턴입니다.

- 검색된 문서의 Entity를 그래프에서 탐색 → 관련 컨텍스트 확장
- "A 서버" 검색 시 관련 DB, 애플리케이션, 담당팀 정보까지 자동으로 가져오기
- 쿼리 라우팅: 질문 유형에 따라 벡터 검색 / 그래프 쿼리 / 하이브리드를 자동 선택

## 답변 품질 비교

RAGAS 평가 프레임워크를 활용해서 정량 비교 예정:

| 방식 | Faithfulness | Answer Relevancy | Context Precision |
|------|-------------|-----------------|-------------------|
| RAG only | ? | ? | ? |
| RAG + Ontology | ? | ? | ? |

## 실무에서 겪는 현실 (삽질 포인트)

- 벡터 검색과 그래프 검색의 결과를 **어떻게 합칠 것인가**가 제일 어려운 문제. 단순 concat? 가중합? 리랭킹?
- 그래프 탐색 depth를 잘못 설정하면 관련 없는 Entity까지 다 끌고 와서 컨텍스트 오염
- 온톨로지 데이터와 RAG 문서의 **정보 불일치** 문제. 위키는 업데이트됐는데 그래프는 안 됐을 때
- 파이프라인이 복잡해질수록 latency 증가. 벡터(50ms) + 그래프(100ms) + 리랭킹(200ms) + LLM(2s) = 사용자가 기다리기 싫어함
- "결국 RAG만 쓸 걸" 하는 팀이 50%는 됨. 온톨로지 구축 비용 대비 효과를 잘 따져야 해요

> **이건 꼭 알아두세요:** RAG + Ontology 통합은 **모든 프로젝트에 필요한 건 아닙니다.** 단순 문서 검색형 챗봇이면 RAG만으로 충분해요. "다단계 관계 추론이 필요한가?", "구조화된 지식 질의가 있는가?"에 대한 답이 "예"일 때만 도입하세요. 아니면 오버엔지니어링입니다.

---

## TODO

- [ ] 통합 파이프라인 전체 코드
- [ ] 벡터+그래프 결합 전략 비교 실험
- [ ] 쿼리 라우팅 구현 (질문 분류 → 검색 방식 선택)
- [ ] RAGAS 평가 결과 (RAG only vs RAG+Ontology)
- [ ] latency 최적화 방법 (병렬 검색, 캐싱)
- [ ] 아키텍처 다이어그램 (mermaid)

---

*시리즈: 온프레미스 LLM 구축 (8/10)*

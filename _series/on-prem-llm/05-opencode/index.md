---
title: "OpenCode 활용 - 보안 걱정 없는 AI 코딩 어시스턴트"
permalink: /series/on-prem-llm/05-opencode/
date: 2026-02-06
excerpt: "로컬 LLM 기반 코딩 어시스턴트 구축과 실사용 후기"
categories:
  - AI-ML
tags:
  - OpenCode
  - AI Agent
  - Coding Assistant
series: "온프레미스 LLM 구축"
series_order: 5
---

## 소개

이제 모델도 올렸으니 뭘 해볼까? **코딩 어시스턴트**를 만들어봅시다. 개발자에게 가장 체감이 큰 유즈케이스거든요. "AI가 코드를 짜준다"는 말만 하면 임원진도 관심을 보입니다.

OpenCode는 오픈소스 AI 코딩 어시스턴트인데, 쉽게 말해서 **"보안 걱정 없는 Claude Code"**입니다. 코드가 사내망 밖으로 안 나가요.

---

## OpenCode 아키텍처

- Go 기반 CLI 도구, 터미널에서 바로 실행
- 로컬 LLM(Ollama/vLLM) 엔드포인트에 연결
- LSP(Language Server Protocol) 통합으로 코드 이해도 향상
- Agent 모드: 파일 읽기/쓰기, 터미널 명령 실행, Git 연동까지 자동
- RAG 연동: 코드베이스 인덱싱 → 프로젝트 맥락을 이해하는 코딩 어시스턴트

## 로컬 LLM 연동

```bash
# Ollama가 돌고 있는 상태에서
# OpenCode 설정에서 엔드포인트 지정
OPENCODE_LLM_ENDPOINT=http://localhost:11434
```

## Claude Code vs OpenCode 솔직 비교

| 항목 | Claude Code | OpenCode + 로컬 LLM |
|------|------------|---------------------|
| 코드 품질 | 최상급 | 70B 기준 60~70% 수준 |
| 응답 속도 | 빠름 (클라우드) | 느림 (로컬 GPU 의존) |
| 보안 | 코드 외부 전송 | 사내망 안에서만 동작 |
| 비용 | 구독료 | GPU 서버 비용 (고정) |
| 커스터마이징 | 제한적 | 파인튜닝/RAG 자유롭게 |

## 실무에서 겪는 현실 (삽질 포인트)

- 8B 모델로 연결하면 코드 품질이 좀 아쉬움. 70B급은 써야 쓸만합니다
- 컨텍스트 윈도우 제한 때문에 큰 파일 전체를 못 넣음 → RAG로 관련 부분만 검색
- Agent 모드에서 `rm -rf` 같은 위험한 명령 실행할 수 있어서 샌드박스 설정 필수
- 로컬 모델이다 보니 응답이 느림 (특히 70B). 사용자 기대치 관리 필요
- 사내 코딩 컨벤션을 프롬프트에 넣어줘야 일관된 코드 스타일 유지 가능

> **이건 꼭 알아두세요:** "로컬 모델로도 Claude Code만큼 되겠지?" → **안 됩니다.** 솔직하게 말하면 70B 양자화 모델 기준으로 Claude 3.5 Sonnet의 60~70% 정도 체감입니다. 하지만 보안 제약이 있는 환경에서는 이게 유일한 선택지이고, 파인튜닝과 RAG를 잘 붙이면 특정 업무에서는 상용 모델보다 나을 수 있습니다.

---

## TODO

- [ ] OpenCode 설치 및 초기 설정 가이드
- [ ] Ollama/vLLM 엔드포인트 연결 설정
- [ ] Agent 모드 샌드박스 설정 방법
- [ ] RAG로 코드베이스 인덱싱하는 방법
- [ ] 사내 코딩 컨벤션 프롬프트 예시
- [ ] 생산성 측정 결과 (Before/After)
- [ ] 스크린샷: 실제 사용 화면

---

*시리즈: 온프레미스 LLM 구축 (5/10)*
